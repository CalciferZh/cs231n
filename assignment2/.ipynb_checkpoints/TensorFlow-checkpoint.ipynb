{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you switch over to that notebook)\n",
    "\n",
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "### Some useful utilities\n",
    "\n",
    ". Remember that our image data is initially N x H x W x C, where:\n",
    "* N is the number of datapoints\n",
    "* H is the height of each image in pixels\n",
    "* W is the height of each image in pixels\n",
    "* C is the number of channels (usually 3: R, G, B)\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, which needs spatial understanding of where the pixels are relative to each other. When we input image data into fully connected affine layers, however, we want each data example to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in TensorFlow -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up. \n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Hinge loss function, and the Adam optimizer being used. \n",
    "\n",
    "Make sure you understand why the parameters of the Linear layer are 5408 and 10.\n",
    "\n",
    "### TensorFlow Details\n",
    "In TensorFlow, much like in our previous notebooks, we'll first specifically initialize our variables, and then our network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). \n",
    "\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "* BatchNorm: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on one epoch\n",
    "While we have defined a graph of operations above, in order to execute TensorFlow Graphs, by feeding them input data and computing the results, we first need to create a `tf.Session` object. A session encapsulates the control and state of the TensorFlow runtime. For more information, see the TensorFlow [Getting started](https://www.tensorflow.org/get_started/get_started) guide.\n",
    "\n",
    "Optionally we can also specify a device context such as `/cpu:0` or `/gpu:0`. For documentation on this behavior see [this TensorFlow guide](https://www.tensorflow.org/tutorials/using_gpu)\n",
    "\n",
    "You should see a validation loss of around 0.4 to 0.6 and an accuracy of 0.30 to 0.35 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 15.4 and accuracy of 0.16\n",
      "Iteration 100: with minibatch training loss = 0.873 and accuracy of 0.33\n",
      "Iteration 200: with minibatch training loss = 0.72 and accuracy of 0.3\n",
      "Iteration 300: with minibatch training loss = 0.525 and accuracy of 0.36\n",
      "Iteration 400: with minibatch training loss = 0.67 and accuracy of 0.27\n",
      "Iteration 500: with minibatch training loss = 0.437 and accuracy of 0.41\n",
      "Iteration 600: with minibatch training loss = 0.469 and accuracy of 0.31\n",
      "Iteration 700: with minibatch training loss = 0.388 and accuracy of 0.39\n",
      "Epoch 1, Overall loss = 0.765 and accuracy of 0.306\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b483148d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 0.493 and accuracy of 0.35\n"
     ]
    }
   ],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,True)\n",
    "        print('Validation')\n",
    "        run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a specific model\n",
    "\n",
    "In this section, we're going to specify a model for you to construct. The goal here isn't to get good performance (that'll be next), but instead to get comfortable with understanding the TensorFlow documentation and configuring your own model. \n",
    "\n",
    "Using the code provided above as guidance, and using the following TensorFlow documentation, specify a model with the following architecture:\n",
    "\n",
    "* 7x7 Convolutional Layer with 32 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* Spatial Batch Normalization Layer (trainable parameters, with scale and centering)\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* Affine layer with 1024 output units\n",
    "* ReLU Activation Layer\n",
    "* Affine layer from 1024 input units to 10 outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define our input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define model\n",
    "def complex_model(X,y,is_training):\n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7,7,3,32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    s_bn_gamma = tf.get_variable(\"s_bn_gamma\", shape=[32])\n",
    "    s_bn_beta = tf.get_variable(\"s_bn_beta\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[8192,1024])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "    W2 = tf.get_variable(\"W2\", shape=[1024,10])\n",
    "    b2 = tf.get_variable(\"b2\", shape=[10])\n",
    "    # define computation graph\n",
    "    l1 = tf.nn.conv2d(X, Wconv1, strides=[1,1,1,1], padding='SAME') + bconv1\n",
    "    l2 = tf.nn.relu(l1)\n",
    "    l2_flat = tf.reshape(l2, [-1,32])\n",
    "    # here we simply let momentum = 0\n",
    "    mean, var = tf.nn.moments(l2_flat, [0])\n",
    "    l3_flat = tf.nn.batch_normalization(l2_flat, mean, var, s_bn_beta, s_bn_gamma, 1e-8)\n",
    "    l3 = tf.reshape(l3_flat, [-1,32,32,32])\n",
    "    # tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
    "    l4 = tf.nn.max_pool(l3, [1,2,2,1], [1,2,2,1], \"VALID\")\n",
    "    l4_flat = tf.reshape(l4, [-1,8192])\n",
    "    l5 = tf.matmul(l4_flat, W1) + b1\n",
    "    l6 = tf.nn.relu(l5)\n",
    "    l7 = tf.matmul(l6, W2) + b2\n",
    "    return l7\n",
    "\n",
    "y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're doing the right thing, use the following tool to check the dimensionality of your output (it should be 64 x 10, since our batches have size 64 and the output of the final affine layer should be 10, corresponding to our 10 classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.02 ms per loop\n",
      "(64, 10)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Now we're going to feed a random batch into the model \n",
    "# and make sure the output is the right size\n",
    "x = np.random.randn(64, 32, 32,3)\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        print(ans.shape)\n",
    "        print(np.array_equal(ans.shape, np.array([64, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following from the run above \n",
    "\n",
    "`(64, 10)`\n",
    "\n",
    "`True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU!\n",
    "\n",
    "Now, we're going to try and start the model under the GPU device, the rest of the code stays unchanged and all our variables and operations will be computed using accelerated code paths. However, if there is no GPU, we get a Python exception and have to rebuild our graph. On a dual-core CPU, you might see around 50-80ms/batch running the above, while the Google Cloud GPUs (run below) should be around 2-5ms/batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 2.92 ms per loop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        with tf.device(\"/gpu:0\") as dev: #\"/cpu:0\" or \"/gpu:0\"\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "            %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")    \n",
    "    # rebuild the graph\n",
    "    # trying to start a GPU throws an exception \n",
    "    # and also trashes the original graph\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that even a simple forward pass like this is significantly faster on the GPU. So for the rest of the assignment (and when you go train your models in assignment 3 and your project!), you should use GPU devices. However, with TensorFlow, the default device is a GPU if one is available, and a CPU otherwise, so we can skip the device specification from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model.\n",
    "\n",
    "Now that you've seen how to define a model and do a single forward pass of some data through it, let's  walk through how you'd actually train one whole epoch over your training data (using the complex_model you created provided above).\n",
    "\n",
    "Make sure you understand how each TensorFlow function used below corresponds to what you implemented in your custom neural network implementation.\n",
    "\n",
    "First, set up an **RMSprop optimizer** (using a 1e-3 learning rate) and a **cross-entropy loss** function. See the TensorFlow documentation for more information\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "#     y_out: is what your model computes\n",
    "#     y: is your TensorFlow variable with label information\n",
    "# Outputs\n",
    "#    mean_loss: a TensorFlow variable (scalar) with numerical loss\n",
    "#    optimizer: a TensorFlow optimizer\n",
    "# This should be ~3 lines of code!\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_out, labels=tf.one_hot(y,10)))\n",
    "optimizer = tf.train.RMSPropOptimizer(1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Below we'll create a session and train the model over one epoch. You should see a loss of 3.0 - 5.0 and an accuracy of 0.2 to 0.3. There will be some variation due to random seeds and differences in initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.34 and accuracy of 0.16\n",
      "Iteration 100: with minibatch training loss = 1.88 and accuracy of 0.3\n",
      "Iteration 200: with minibatch training loss = 1.34 and accuracy of 0.53\n",
      "Iteration 300: with minibatch training loss = 1.61 and accuracy of 0.42\n",
      "Iteration 400: with minibatch training loss = 1.39 and accuracy of 0.42\n",
      "Iteration 500: with minibatch training loss = 1.37 and accuracy of 0.55\n",
      "Iteration 600: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 700: with minibatch training loss = 1.03 and accuracy of 0.61\n",
      "Epoch 1, Overall loss = 1.53 and accuracy of 0.451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.5280525456642617, 0.4513673469387755)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the model.\n",
    "\n",
    "Let's see the train and test code in action -- feel free to use these methods when evaluating the models you develop below. You should see a loss of 1.5 to 2.0 with an accuracy of 0.3 to 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 1.22 and accuracy of 0.579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2150139007568359, 0.57899999999999996)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a _great_ model on CIFAR-10!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves ** >= 70% accuracy on the validation set** of CIFAR-10. You can use the `run_model` function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Use TensorFlow Scope**: Use TensorFlow scope and/or [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) to make it easier to write deeper networks. See [this tutorial](https://www.tensorflow.org/tutorials/layers) for making how to use `tf.layers`. \n",
    "- **Use Learning Rate Decay**: [As the notes point out](http://cs231n.github.io/neural-networks-3/#anneal), decaying the learning rate might help the model converge. Feel free to decay every epoch, when loss doesn't change over an entire epoch, or any other heuristic you find appropriate. See the [Tensorflow documentation](https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate) for learning rate decay.\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use [Dropout as in the TensorFlow MNIST tutorial](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and we'll save the test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at **>= 70% accuracy on the validation set**. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training and validation set accuracies for your final trained network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input->conv1->relu->conv2->relu->maxpool->conv3->relu->conv4->relu->maxpool->fc1->bn->fc2->output\n",
    "# 32x32x3->32x32x64->32x32x64--->16x16x64->16x16x128->16x16x256------>8x8x256--------------->output\n",
    "def my_model(X,y,is_training):\n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[3,3,3,64])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[64])\n",
    "    \n",
    "    Wconv2 = tf.get_variable(\"Wconv2\", shape=[3,3,64,64])\n",
    "    bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n",
    "    \n",
    "    Wconv3 = tf.get_variable(\"Wconv3\", shape=[3,3,64,128])\n",
    "    bconv3 = tf.get_variable(\"bconv3\", shape=[128])\n",
    "    \n",
    "    Wconv4 = tf.get_variable(\"Wconv4\", shape=[3,3,128,256])\n",
    "    bconv4 = tf.get_variable(\"bconv4\", shape=[256])\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", shape=[16384,1024])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "    \n",
    "    W2 = tf.get_variable(\"W2\", shape=[1024,10])\n",
    "    b2 = tf.get_variable(\"b2\", shape=[10])\n",
    "    \n",
    "    gamma = tf.get_variable('gamma', shape=[1])\n",
    "    beta = tf.get_variable('beta', shape=[1])\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # define computation graph\n",
    "    l1 = tf.nn.conv2d(X, Wconv1, strides=[1,1,1,1], padding='SAME') + bconv1\n",
    "    l2 = tf.nn.relu(l1)\n",
    "    l3 = tf.nn.conv2d(l2, Wconv2, strides=[1,1,1,1], padding='SAME') + bconv2\n",
    "    l4 = tf.nn.relu(l3)\n",
    "    l5 = tf.nn.max_pool(l4, [1,2,2,1], [1,2,2,1], 'VALID')\n",
    "    l6 = tf.nn.conv2d(l5, Wconv3, strides=[1,1,1,1], padding='SAME') + bconv3\n",
    "    l7 = tf.nn.relu(l6)\n",
    "    l8 = tf.nn.conv2d(l7, Wconv4, strides=[1,1,1,1], padding='SAME') + bconv4\n",
    "    l9 = tf.nn.relu(l8)\n",
    "    l10 = tf.nn.max_pool(l9, [1,2,2,1], [1,2,2,1], 'VALID')\n",
    "    l10_flat = tf.reshape(l10, [-1, 16384])\n",
    "    l11 = tf.matmul(l10_flat, W1) + b1\n",
    "    mean, var = tf.nn.moments(l11, [0])\n",
    "    l12 = tf.nn.batch_normalization(l11, mean, var, beta, gamma, eps)\n",
    "    l13 = tf.matmul(l12, W2) + b2\n",
    "    return l13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 4.47 and accuracy of 0.12\n",
      "Iteration 100: with minibatch training loss = 5.14 and accuracy of 0.11\n",
      "Iteration 200: with minibatch training loss = 1.79 and accuracy of 0.31\n",
      "Iteration 300: with minibatch training loss = 1.59 and accuracy of 0.42\n",
      "Iteration 400: with minibatch training loss = 1.73 and accuracy of 0.38\n",
      "Iteration 500: with minibatch training loss = 1.73 and accuracy of 0.5\n",
      "Iteration 600: with minibatch training loss = 1.12 and accuracy of 0.55\n",
      "Iteration 700: with minibatch training loss = 1.2 and accuracy of 0.59\n",
      "Epoch 1, Overall loss = 2.18 and accuracy of 0.423\n",
      "Iteration 800: with minibatch training loss = 1.6 and accuracy of 0.47\n",
      "Iteration 900: with minibatch training loss = 1.13 and accuracy of 0.64\n",
      "Iteration 1000: with minibatch training loss = 1.34 and accuracy of 0.59\n",
      "Iteration 1100: with minibatch training loss = 1.28 and accuracy of 0.58\n",
      "Iteration 1200: with minibatch training loss = 1.38 and accuracy of 0.53\n",
      "Iteration 1300: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 1400: with minibatch training loss = 1.3 and accuracy of 0.59\n",
      "Iteration 1500: with minibatch training loss = 1.36 and accuracy of 0.64\n",
      "Epoch 2, Overall loss = 1.22 and accuracy of 0.641\n",
      "Iteration 1600: with minibatch training loss = 0.906 and accuracy of 0.69\n",
      "Iteration 1700: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 1800: with minibatch training loss = 0.957 and accuracy of 0.69\n",
      "Iteration 1900: with minibatch training loss = 1.17 and accuracy of 0.66\n",
      "Iteration 2000: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 2100: with minibatch training loss = 1.17 and accuracy of 0.64\n",
      "Iteration 2200: with minibatch training loss = 0.95 and accuracy of 0.73\n",
      "Epoch 3, Overall loss = 1.11 and accuracy of 0.692\n",
      "Iteration 2300: with minibatch training loss = 1.26 and accuracy of 0.62\n",
      "Iteration 2400: with minibatch training loss = 0.892 and accuracy of 0.8\n",
      "Iteration 2500: with minibatch training loss = 1.15 and accuracy of 0.64\n",
      "Iteration 2600: with minibatch training loss = 0.888 and accuracy of 0.78\n",
      "Iteration 2700: with minibatch training loss = 0.929 and accuracy of 0.8\n",
      "Iteration 2800: with minibatch training loss = 0.84 and accuracy of 0.8\n",
      "Iteration 2900: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 3000: with minibatch training loss = 0.871 and accuracy of 0.8\n",
      "Epoch 4, Overall loss = 1.03 and accuracy of 0.724\n",
      "Iteration 3100: with minibatch training loss = 0.843 and accuracy of 0.81\n",
      "Iteration 3200: with minibatch training loss = 1.11 and accuracy of 0.75\n",
      "Iteration 3300: with minibatch training loss = 0.996 and accuracy of 0.75\n",
      "Iteration 3400: with minibatch training loss = 0.695 and accuracy of 0.88\n",
      "Iteration 3500: with minibatch training loss = 0.994 and accuracy of 0.69\n",
      "Iteration 3600: with minibatch training loss = 0.874 and accuracy of 0.75\n",
      "Iteration 3700: with minibatch training loss = 0.754 and accuracy of 0.81\n",
      "Iteration 3800: with minibatch training loss = 0.759 and accuracy of 0.84\n",
      "Epoch 5, Overall loss = 0.967 and accuracy of 0.75\n",
      "Iteration 3900: with minibatch training loss = 0.719 and accuracy of 0.84\n",
      "Iteration 4000: with minibatch training loss = 0.927 and accuracy of 0.75\n",
      "Iteration 4100: with minibatch training loss = 1.11 and accuracy of 0.81\n",
      "Iteration 4200: with minibatch training loss = 0.911 and accuracy of 0.7\n",
      "Iteration 4300: with minibatch training loss = 0.832 and accuracy of 0.8\n",
      "Iteration 4400: with minibatch training loss = 1.22 and accuracy of 0.75\n",
      "Iteration 4500: with minibatch training loss = 0.766 and accuracy of 0.75\n",
      "Epoch 6, Overall loss = 0.922 and accuracy of 0.772\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.08 and accuracy of 0.744\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 4.4 and accuracy of 0.11\n",
      "Iteration 100: with minibatch training loss = 3.78 and accuracy of 0.23\n",
      "Iteration 200: with minibatch training loss = 2.06 and accuracy of 0.23\n",
      "Iteration 300: with minibatch training loss = 1.48 and accuracy of 0.5\n",
      "Iteration 400: with minibatch training loss = 1.43 and accuracy of 0.52\n",
      "Iteration 500: with minibatch training loss = 1.46 and accuracy of 0.52\n",
      "Iteration 600: with minibatch training loss = 1.12 and accuracy of 0.58\n",
      "Iteration 700: with minibatch training loss = 1.16 and accuracy of 0.61\n",
      "Epoch 1, Overall loss = 2.2 and accuracy of 0.444\n",
      "Iteration 800: with minibatch training loss = 0.811 and accuracy of 0.75\n",
      "Iteration 900: with minibatch training loss = 1.07 and accuracy of 0.59\n",
      "Iteration 1000: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 1100: with minibatch training loss = 1.41 and accuracy of 0.58\n",
      "Iteration 1200: with minibatch training loss = 0.988 and accuracy of 0.67\n",
      "Iteration 1300: with minibatch training loss = 1.09 and accuracy of 0.67\n",
      "Iteration 1400: with minibatch training loss = 0.889 and accuracy of 0.7\n",
      "Iteration 1500: with minibatch training loss = 0.823 and accuracy of 0.77\n",
      "Epoch 2, Overall loss = 1.02 and accuracy of 0.674\n",
      "Iteration 1600: with minibatch training loss = 0.806 and accuracy of 0.62\n",
      "Iteration 1700: with minibatch training loss = 0.886 and accuracy of 0.75\n",
      "Iteration 1800: with minibatch training loss = 0.987 and accuracy of 0.72\n",
      "Iteration 1900: with minibatch training loss = 0.892 and accuracy of 0.78\n",
      "Iteration 2000: with minibatch training loss = 0.879 and accuracy of 0.7\n",
      "Iteration 2100: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 2200: with minibatch training loss = 0.893 and accuracy of 0.73\n",
      "Epoch 3, Overall loss = 0.872 and accuracy of 0.739\n",
      "Iteration 2300: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 2400: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 2500: with minibatch training loss = 1.25 and accuracy of 0.64\n",
      "Iteration 2600: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 2700: with minibatch training loss = 0.831 and accuracy of 0.8\n",
      "Iteration 2800: with minibatch training loss = 0.83 and accuracy of 0.8\n",
      "Iteration 2900: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 3000: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Epoch 4, Overall loss = 0.782 and accuracy of 0.785\n",
      "Iteration 3100: with minibatch training loss = 0.647 and accuracy of 0.83\n",
      "Iteration 3200: with minibatch training loss = 0.566 and accuracy of 0.88\n",
      "Iteration 3300: with minibatch training loss = 0.675 and accuracy of 0.84\n",
      "Iteration 3400: with minibatch training loss = 0.553 and accuracy of 0.86\n",
      "Iteration 3500: with minibatch training loss = 0.691 and accuracy of 0.84\n",
      "Iteration 3600: with minibatch training loss = 0.709 and accuracy of 0.83\n",
      "Iteration 3700: with minibatch training loss = 0.627 and accuracy of 0.86\n",
      "Iteration 3800: with minibatch training loss = 0.577 and accuracy of 0.88\n",
      "Epoch 5, Overall loss = 0.711 and accuracy of 0.821\n",
      "Iteration 3900: with minibatch training loss = 0.713 and accuracy of 0.84\n",
      "Iteration 4000: with minibatch training loss = 0.864 and accuracy of 0.72\n",
      "Iteration 4100: with minibatch training loss = 0.924 and accuracy of 0.72\n",
      "Iteration 4200: with minibatch training loss = 0.521 and accuracy of 0.86\n",
      "Iteration 4300: with minibatch training loss = 0.552 and accuracy of 0.89\n",
      "Iteration 4400: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 4500: with minibatch training loss = 0.583 and accuracy of 0.89\n",
      "Epoch 6, Overall loss = 0.655 and accuracy of 0.85\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.17 and accuracy of 0.753\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.77 and accuracy of 0.062\n",
      "Iteration 100: with minibatch training loss = 1.84 and accuracy of 0.44\n",
      "Iteration 200: with minibatch training loss = 1.93 and accuracy of 0.25\n",
      "Iteration 300: with minibatch training loss = 1.63 and accuracy of 0.39\n",
      "Iteration 400: with minibatch training loss = 1.31 and accuracy of 0.53\n",
      "Iteration 500: with minibatch training loss = 1.12 and accuracy of 0.59\n",
      "Iteration 600: with minibatch training loss = 1.17 and accuracy of 0.53\n",
      "Iteration 700: with minibatch training loss = 1.13 and accuracy of 0.59\n",
      "Epoch 1, Overall loss = 1.49 and accuracy of 0.496\n",
      "Iteration 800: with minibatch training loss = 1.47 and accuracy of 0.59\n",
      "Iteration 900: with minibatch training loss = 0.871 and accuracy of 0.72\n",
      "Iteration 1000: with minibatch training loss = 0.651 and accuracy of 0.77\n",
      "Iteration 1100: with minibatch training loss = 0.794 and accuracy of 0.75\n",
      "Iteration 1200: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 1300: with minibatch training loss = 0.662 and accuracy of 0.73\n",
      "Iteration 1400: with minibatch training loss = 0.86 and accuracy of 0.72\n",
      "Iteration 1500: with minibatch training loss = 0.843 and accuracy of 0.66\n",
      "Epoch 2, Overall loss = 0.886 and accuracy of 0.704\n",
      "Iteration 1600: with minibatch training loss = 0.77 and accuracy of 0.72\n",
      "Iteration 1700: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 1800: with minibatch training loss = 0.497 and accuracy of 0.83\n",
      "Iteration 1900: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 2000: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 2100: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 2200: with minibatch training loss = 0.656 and accuracy of 0.78\n",
      "Epoch 3, Overall loss = 0.703 and accuracy of 0.77\n",
      "Iteration 2300: with minibatch training loss = 0.671 and accuracy of 0.8\n",
      "Iteration 2400: with minibatch training loss = 0.646 and accuracy of 0.8\n",
      "Iteration 2500: with minibatch training loss = 0.372 and accuracy of 0.86\n",
      "Iteration 2600: with minibatch training loss = 0.394 and accuracy of 0.88\n",
      "Iteration 2700: with minibatch training loss = 0.394 and accuracy of 0.88\n",
      "Iteration 2800: with minibatch training loss = 0.465 and accuracy of 0.92\n",
      "Iteration 2900: with minibatch training loss = 0.555 and accuracy of 0.83\n",
      "Iteration 3000: with minibatch training loss = 0.408 and accuracy of 0.91\n",
      "Epoch 4, Overall loss = 0.567 and accuracy of 0.821\n",
      "Iteration 3100: with minibatch training loss = 0.651 and accuracy of 0.75\n",
      "Iteration 3200: with minibatch training loss = 0.598 and accuracy of 0.86\n",
      "Iteration 3300: with minibatch training loss = 0.42 and accuracy of 0.88\n",
      "Iteration 3400: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 3500: with minibatch training loss = 0.528 and accuracy of 0.83\n",
      "Iteration 3600: with minibatch training loss = 0.485 and accuracy of 0.83\n",
      "Iteration 3700: with minibatch training loss = 0.362 and accuracy of 0.91\n",
      "Iteration 3800: with minibatch training loss = 0.312 and accuracy of 0.89\n",
      "Epoch 5, Overall loss = 0.45 and accuracy of 0.863\n",
      "Iteration 3900: with minibatch training loss = 0.405 and accuracy of 0.88\n",
      "Iteration 4000: with minibatch training loss = 0.281 and accuracy of 0.92\n",
      "Iteration 4100: with minibatch training loss = 0.579 and accuracy of 0.91\n",
      "Iteration 4200: with minibatch training loss = 0.383 and accuracy of 0.92\n",
      "Iteration 4300: with minibatch training loss = 0.242 and accuracy of 0.94\n",
      "Iteration 4400: with minibatch training loss = 0.326 and accuracy of 0.91\n",
      "Iteration 4500: with minibatch training loss = 0.343 and accuracy of 0.89\n",
      "Epoch 6, Overall loss = 0.364 and accuracy of 0.895\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.35 and accuracy of 0.77\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 4.13 and accuracy of 0.11\n",
      "Iteration 100: with minibatch training loss = 1.67 and accuracy of 0.39\n",
      "Iteration 200: with minibatch training loss = 1.59 and accuracy of 0.42\n",
      "Iteration 300: with minibatch training loss = 1.6 and accuracy of 0.5\n",
      "Iteration 400: with minibatch training loss = 1.29 and accuracy of 0.56\n",
      "Iteration 500: with minibatch training loss = 0.882 and accuracy of 0.67\n",
      "Iteration 600: with minibatch training loss = 0.918 and accuracy of 0.72\n",
      "Iteration 700: with minibatch training loss = 1.09 and accuracy of 0.59\n",
      "Epoch 1, Overall loss = 1.37 and accuracy of 0.535\n",
      "Iteration 800: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 900: with minibatch training loss = 1.23 and accuracy of 0.62\n",
      "Iteration 1000: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 1100: with minibatch training loss = 0.723 and accuracy of 0.73\n",
      "Iteration 1200: with minibatch training loss = 0.629 and accuracy of 0.8\n",
      "Iteration 1300: with minibatch training loss = 0.863 and accuracy of 0.75\n",
      "Iteration 1400: with minibatch training loss = 0.66 and accuracy of 0.84\n",
      "Iteration 1500: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Epoch 2, Overall loss = 0.806 and accuracy of 0.74\n",
      "Iteration 1600: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 1700: with minibatch training loss = 0.617 and accuracy of 0.77\n",
      "Iteration 1800: with minibatch training loss = 0.468 and accuracy of 0.88\n",
      "Iteration 1900: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 2000: with minibatch training loss = 0.636 and accuracy of 0.77\n",
      "Iteration 2100: with minibatch training loss = 0.623 and accuracy of 0.83\n",
      "Iteration 2200: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Epoch 3, Overall loss = 0.622 and accuracy of 0.812\n",
      "Iteration 2300: with minibatch training loss = 0.795 and accuracy of 0.75\n",
      "Iteration 2400: with minibatch training loss = 0.53 and accuracy of 0.88\n",
      "Iteration 2500: with minibatch training loss = 0.702 and accuracy of 0.72\n",
      "Iteration 2600: with minibatch training loss = 0.314 and accuracy of 0.91\n",
      "Iteration 2700: with minibatch training loss = 0.556 and accuracy of 0.86\n",
      "Iteration 2800: with minibatch training loss = 0.504 and accuracy of 0.83\n",
      "Iteration 2900: with minibatch training loss = 0.434 and accuracy of 0.92\n",
      "Iteration 3000: with minibatch training loss = 0.466 and accuracy of 0.86\n",
      "Epoch 4, Overall loss = 0.493 and accuracy of 0.864\n",
      "Iteration 3100: with minibatch training loss = 0.302 and accuracy of 0.92\n",
      "Iteration 3200: with minibatch training loss = 0.596 and accuracy of 0.88\n",
      "Iteration 3300: with minibatch training loss = 0.349 and accuracy of 0.91\n",
      "Iteration 3400: with minibatch training loss = 0.472 and accuracy of 0.94\n",
      "Iteration 3500: with minibatch training loss = 0.484 and accuracy of 0.88\n",
      "Iteration 3600: with minibatch training loss = 0.306 and accuracy of 0.94\n",
      "Iteration 3700: with minibatch training loss = 0.319 and accuracy of 0.91\n",
      "Iteration 3800: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Epoch 5, Overall loss = 0.396 and accuracy of 0.902\n",
      "Iteration 3900: with minibatch training loss = 0.363 and accuracy of 0.88\n",
      "Iteration 4000: with minibatch training loss = 0.249 and accuracy of 0.97\n",
      "Iteration 4100: with minibatch training loss = 0.546 and accuracy of 0.91\n",
      "Iteration 4200: with minibatch training loss = 0.217 and accuracy of 0.97\n",
      "Iteration 4300: with minibatch training loss = 0.369 and accuracy of 0.92\n",
      "Iteration 4400: with minibatch training loss = 0.29 and accuracy of 0.94\n",
      "Iteration 4500: with minibatch training loss = 0.297 and accuracy of 0.95\n",
      "Epoch 6, Overall loss = 0.342 and accuracy of 0.922\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.12 and accuracy of 0.777\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.61 and accuracy of 0.11\n",
      "Iteration 100: with minibatch training loss = 2.07 and accuracy of 0.36\n",
      "Iteration 200: with minibatch training loss = 1.14 and accuracy of 0.55\n",
      "Iteration 300: with minibatch training loss = 1.36 and accuracy of 0.58\n",
      "Iteration 400: with minibatch training loss = 1.3 and accuracy of 0.56\n",
      "Iteration 500: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 600: with minibatch training loss = 1.03 and accuracy of 0.66\n",
      "Iteration 700: with minibatch training loss = 0.931 and accuracy of 0.72\n",
      "Epoch 1, Overall loss = 1.26 and accuracy of 0.561\n",
      "Iteration 800: with minibatch training loss = 0.916 and accuracy of 0.67\n",
      "Iteration 900: with minibatch training loss = 0.675 and accuracy of 0.72\n",
      "Iteration 1000: with minibatch training loss = 0.831 and accuracy of 0.72\n",
      "Iteration 1100: with minibatch training loss = 0.812 and accuracy of 0.7\n",
      "Iteration 1200: with minibatch training loss = 0.561 and accuracy of 0.77\n",
      "Iteration 1300: with minibatch training loss = 0.704 and accuracy of 0.77\n",
      "Iteration 1400: with minibatch training loss = 0.945 and accuracy of 0.66\n",
      "Iteration 1500: with minibatch training loss = 0.398 and accuracy of 0.88\n",
      "Epoch 2, Overall loss = 0.745 and accuracy of 0.746\n",
      "Iteration 1600: with minibatch training loss = 0.828 and accuracy of 0.73\n",
      "Iteration 1700: with minibatch training loss = 0.565 and accuracy of 0.84\n",
      "Iteration 1800: with minibatch training loss = 0.667 and accuracy of 0.78\n",
      "Iteration 1900: with minibatch training loss = 0.61 and accuracy of 0.78\n",
      "Iteration 2000: with minibatch training loss = 0.675 and accuracy of 0.84\n",
      "Iteration 2100: with minibatch training loss = 0.328 and accuracy of 0.89\n",
      "Iteration 2200: with minibatch training loss = 0.51 and accuracy of 0.88\n",
      "Epoch 3, Overall loss = 0.551 and accuracy of 0.819\n",
      "Iteration 2300: with minibatch training loss = 0.21 and accuracy of 0.97\n",
      "Iteration 2400: with minibatch training loss = 0.343 and accuracy of 0.89\n",
      "Iteration 2500: with minibatch training loss = 0.235 and accuracy of 0.95\n",
      "Iteration 2600: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 2700: with minibatch training loss = 0.316 and accuracy of 0.91\n",
      "Iteration 2800: with minibatch training loss = 0.375 and accuracy of 0.89\n",
      "Iteration 2900: with minibatch training loss = 0.524 and accuracy of 0.83\n",
      "Iteration 3000: with minibatch training loss = 0.322 and accuracy of 0.91\n",
      "Epoch 4, Overall loss = 0.401 and accuracy of 0.871\n",
      "Iteration 3100: with minibatch training loss = 0.402 and accuracy of 0.91\n",
      "Iteration 3200: with minibatch training loss = 0.245 and accuracy of 0.91\n",
      "Iteration 3300: with minibatch training loss = 0.128 and accuracy of 0.95\n",
      "Iteration 3400: with minibatch training loss = 0.472 and accuracy of 0.84\n",
      "Iteration 3500: with minibatch training loss = 0.415 and accuracy of 0.83\n",
      "Iteration 3600: with minibatch training loss = 0.213 and accuracy of 0.95\n",
      "Iteration 3700: with minibatch training loss = 0.222 and accuracy of 0.97\n",
      "Iteration 3800: with minibatch training loss = 0.129 and accuracy of 0.97\n",
      "Epoch 5, Overall loss = 0.29 and accuracy of 0.908\n",
      "Iteration 3900: with minibatch training loss = 0.242 and accuracy of 0.92\n",
      "Iteration 4000: with minibatch training loss = 0.194 and accuracy of 0.92\n",
      "Iteration 4100: with minibatch training loss = 0.0909 and accuracy of 0.98\n",
      "Iteration 4200: with minibatch training loss = 0.161 and accuracy of 0.94\n",
      "Iteration 4300: with minibatch training loss = 0.169 and accuracy of 0.95\n",
      "Iteration 4400: with minibatch training loss = 0.402 and accuracy of 0.89\n",
      "Iteration 4500: with minibatch training loss = 0.0962 and accuracy of 0.98\n",
      "Epoch 6, Overall loss = 0.221 and accuracy of 0.931\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.06 and accuracy of 0.791\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.57 and accuracy of 0.062\n",
      "Iteration 100: with minibatch training loss = 1.58 and accuracy of 0.44\n",
      "Iteration 200: with minibatch training loss = 1.31 and accuracy of 0.45\n",
      "Iteration 300: with minibatch training loss = 1.47 and accuracy of 0.42\n",
      "Iteration 400: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 500: with minibatch training loss = 1.02 and accuracy of 0.58\n",
      "Iteration 600: with minibatch training loss = 0.925 and accuracy of 0.67\n",
      "Iteration 700: with minibatch training loss = 0.962 and accuracy of 0.66\n",
      "Epoch 1, Overall loss = 1.23 and accuracy of 0.569\n",
      "Iteration 800: with minibatch training loss = 0.844 and accuracy of 0.72\n",
      "Iteration 900: with minibatch training loss = 0.68 and accuracy of 0.77\n",
      "Iteration 1000: with minibatch training loss = 0.935 and accuracy of 0.64\n",
      "Iteration 1100: with minibatch training loss = 0.725 and accuracy of 0.75\n",
      "Iteration 1200: with minibatch training loss = 0.49 and accuracy of 0.8\n",
      "Iteration 1300: with minibatch training loss = 0.803 and accuracy of 0.73\n",
      "Iteration 1400: with minibatch training loss = 0.597 and accuracy of 0.77\n",
      "Iteration 1500: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Epoch 2, Overall loss = 0.718 and accuracy of 0.754\n",
      "Iteration 1600: with minibatch training loss = 0.605 and accuracy of 0.78\n",
      "Iteration 1700: with minibatch training loss = 0.763 and accuracy of 0.75\n",
      "Iteration 1800: with minibatch training loss = 0.705 and accuracy of 0.83\n",
      "Iteration 1900: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 2000: with minibatch training loss = 0.578 and accuracy of 0.8\n",
      "Iteration 2100: with minibatch training loss = 0.431 and accuracy of 0.88\n",
      "Iteration 2200: with minibatch training loss = 0.708 and accuracy of 0.73\n",
      "Epoch 3, Overall loss = 0.523 and accuracy of 0.824\n",
      "Iteration 2300: with minibatch training loss = 0.505 and accuracy of 0.78\n",
      "Iteration 2400: with minibatch training loss = 0.475 and accuracy of 0.88\n",
      "Iteration 2500: with minibatch training loss = 0.21 and accuracy of 0.94\n",
      "Iteration 2600: with minibatch training loss = 0.309 and accuracy of 0.88\n",
      "Iteration 2700: with minibatch training loss = 0.454 and accuracy of 0.83\n",
      "Iteration 2800: with minibatch training loss = 0.44 and accuracy of 0.88\n",
      "Iteration 2900: with minibatch training loss = 0.237 and accuracy of 0.91\n",
      "Iteration 3000: with minibatch training loss = 0.186 and accuracy of 0.92\n",
      "Epoch 4, Overall loss = 0.371 and accuracy of 0.875\n",
      "Iteration 3100: with minibatch training loss = 0.274 and accuracy of 0.89\n",
      "Iteration 3200: with minibatch training loss = 0.3 and accuracy of 0.95\n",
      "Iteration 3300: with minibatch training loss = 0.201 and accuracy of 0.94\n",
      "Iteration 3400: with minibatch training loss = 0.268 and accuracy of 0.91\n",
      "Iteration 3500: with minibatch training loss = 0.32 and accuracy of 0.92\n",
      "Iteration 3600: with minibatch training loss = 0.289 and accuracy of 0.94\n",
      "Iteration 3700: with minibatch training loss = 0.204 and accuracy of 0.94\n",
      "Iteration 3800: with minibatch training loss = 0.345 and accuracy of 0.84\n",
      "Epoch 5, Overall loss = 0.257 and accuracy of 0.914\n",
      "Iteration 3900: with minibatch training loss = 0.144 and accuracy of 0.94\n",
      "Iteration 4000: with minibatch training loss = 0.255 and accuracy of 0.94\n",
      "Iteration 4100: with minibatch training loss = 0.131 and accuracy of 0.95\n",
      "Iteration 4200: with minibatch training loss = 0.249 and accuracy of 0.91\n",
      "Iteration 4300: with minibatch training loss = 0.207 and accuracy of 0.91\n",
      "Iteration 4400: with minibatch training loss = 0.0708 and accuracy of 0.98\n",
      "Iteration 4500: with minibatch training loss = 0.353 and accuracy of 0.88\n",
      "Epoch 6, Overall loss = 0.189 and accuracy of 0.934\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.12 and accuracy of 0.775\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 3.77 and accuracy of 0.2\n",
      "Iteration 100: with minibatch training loss = 2.85 and accuracy of 0.34\n",
      "Iteration 200: with minibatch training loss = 1.83 and accuracy of 0.42\n",
      "Iteration 300: with minibatch training loss = 1.51 and accuracy of 0.5\n",
      "Iteration 400: with minibatch training loss = 1.29 and accuracy of 0.55\n",
      "Iteration 500: with minibatch training loss = 1.25 and accuracy of 0.56\n",
      "Iteration 600: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 700: with minibatch training loss = 1.06 and accuracy of 0.66\n",
      "Epoch 1, Overall loss = 1.61 and accuracy of 0.498\n",
      "Iteration 800: with minibatch training loss = 1.24 and accuracy of 0.56\n",
      "Iteration 900: with minibatch training loss = 1.06 and accuracy of 0.62\n",
      "Iteration 1000: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 1100: with minibatch training loss = 1.03 and accuracy of 0.64\n",
      "Iteration 1200: with minibatch training loss = 0.786 and accuracy of 0.73\n",
      "Iteration 1300: with minibatch training loss = 0.946 and accuracy of 0.7\n",
      "Iteration 1400: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 1500: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Epoch 2, Overall loss = 0.903 and accuracy of 0.698\n",
      "Iteration 1600: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 1700: with minibatch training loss = 0.808 and accuracy of 0.73\n",
      "Iteration 1800: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 1900: with minibatch training loss = 0.928 and accuracy of 0.72\n",
      "Iteration 2000: with minibatch training loss = 0.484 and accuracy of 0.86\n",
      "Iteration 2100: with minibatch training loss = 0.62 and accuracy of 0.78\n",
      "Iteration 2200: with minibatch training loss = 0.577 and accuracy of 0.78\n",
      "Epoch 3, Overall loss = 0.615 and accuracy of 0.797\n",
      "Iteration 2300: with minibatch training loss = 0.46 and accuracy of 0.86\n",
      "Iteration 2400: with minibatch training loss = 0.745 and accuracy of 0.7\n",
      "Iteration 2500: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 2600: with minibatch training loss = 0.487 and accuracy of 0.84\n",
      "Iteration 2700: with minibatch training loss = 0.675 and accuracy of 0.84\n",
      "Iteration 2800: with minibatch training loss = 0.267 and accuracy of 0.92\n",
      "Iteration 2900: with minibatch training loss = 0.478 and accuracy of 0.81\n",
      "Iteration 3000: with minibatch training loss = 0.216 and accuracy of 0.91\n",
      "Epoch 4, Overall loss = 0.39 and accuracy of 0.875\n",
      "Iteration 3100: with minibatch training loss = 0.536 and accuracy of 0.81\n",
      "Iteration 3200: with minibatch training loss = 0.247 and accuracy of 0.94\n",
      "Iteration 3300: with minibatch training loss = 0.253 and accuracy of 0.94\n",
      "Iteration 3400: with minibatch training loss = 0.362 and accuracy of 0.89\n",
      "Iteration 3500: with minibatch training loss = 0.282 and accuracy of 0.89\n",
      "Iteration 3600: with minibatch training loss = 0.177 and accuracy of 0.92\n",
      "Iteration 3700: with minibatch training loss = 0.224 and accuracy of 0.95\n",
      "Iteration 3800: with minibatch training loss = 0.161 and accuracy of 0.94\n",
      "Epoch 5, Overall loss = 0.256 and accuracy of 0.917\n",
      "Iteration 3900: with minibatch training loss = 0.336 and accuracy of 0.88\n",
      "Iteration 4000: with minibatch training loss = 0.141 and accuracy of 0.97\n",
      "Iteration 4100: with minibatch training loss = 0.266 and accuracy of 0.89\n",
      "Iteration 4200: with minibatch training loss = 0.244 and accuracy of 0.94\n",
      "Iteration 4300: with minibatch training loss = 0.171 and accuracy of 0.92\n",
      "Iteration 4400: with minibatch training loss = 0.13 and accuracy of 0.95\n",
      "Iteration 4500: with minibatch training loss = 0.49 and accuracy of 0.86\n",
      "Epoch 6, Overall loss = 0.206 and accuracy of 0.933\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.9 and accuracy of 0.667\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 4.08 and accuracy of 0.094\n",
      "Iteration 100: with minibatch training loss = 2.56 and accuracy of 0.38\n",
      "Iteration 200: with minibatch training loss = 1.96 and accuracy of 0.3\n",
      "Iteration 300: with minibatch training loss = 1.5 and accuracy of 0.48\n",
      "Iteration 400: with minibatch training loss = 1.18 and accuracy of 0.58\n",
      "Iteration 500: with minibatch training loss = 1.38 and accuracy of 0.55\n",
      "Iteration 600: with minibatch training loss = 1.09 and accuracy of 0.58\n",
      "Iteration 700: with minibatch training loss = 0.956 and accuracy of 0.61\n",
      "Epoch 1, Overall loss = 1.52 and accuracy of 0.499\n",
      "Iteration 800: with minibatch training loss = 0.866 and accuracy of 0.67\n",
      "Iteration 900: with minibatch training loss = 0.92 and accuracy of 0.7\n",
      "Iteration 1000: with minibatch training loss = 0.834 and accuracy of 0.67\n",
      "Iteration 1100: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 1200: with minibatch training loss = 0.712 and accuracy of 0.67\n",
      "Iteration 1300: with minibatch training loss = 0.782 and accuracy of 0.7\n",
      "Iteration 1400: with minibatch training loss = 0.91 and accuracy of 0.69\n",
      "Iteration 1500: with minibatch training loss = 0.708 and accuracy of 0.77\n",
      "Epoch 2, Overall loss = 0.893 and accuracy of 0.694\n",
      "Iteration 1600: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 1700: with minibatch training loss = 0.717 and accuracy of 0.77\n",
      "Iteration 1800: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 1900: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 2000: with minibatch training loss = 0.398 and accuracy of 0.84\n",
      "Iteration 2100: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 2200: with minibatch training loss = 0.518 and accuracy of 0.81\n",
      "Epoch 3, Overall loss = 0.606 and accuracy of 0.796\n",
      "Iteration 2300: with minibatch training loss = 0.457 and accuracy of 0.81\n",
      "Iteration 2400: with minibatch training loss = 0.344 and accuracy of 0.86\n",
      "Iteration 2500: with minibatch training loss = 0.26 and accuracy of 0.92\n",
      "Iteration 2600: with minibatch training loss = 0.24 and accuracy of 0.94\n",
      "Iteration 2700: with minibatch training loss = 0.34 and accuracy of 0.91\n",
      "Iteration 2800: with minibatch training loss = 0.316 and accuracy of 0.86\n",
      "Iteration 2900: with minibatch training loss = 0.304 and accuracy of 0.92\n",
      "Iteration 3000: with minibatch training loss = 0.4 and accuracy of 0.88\n",
      "Epoch 4, Overall loss = 0.375 and accuracy of 0.876\n",
      "Iteration 3100: with minibatch training loss = 0.259 and accuracy of 0.92\n",
      "Iteration 3200: with minibatch training loss = 0.244 and accuracy of 0.92\n",
      "Iteration 3300: with minibatch training loss = 0.206 and accuracy of 0.95\n",
      "Iteration 3400: with minibatch training loss = 0.257 and accuracy of 0.94\n",
      "Iteration 3500: with minibatch training loss = 0.267 and accuracy of 0.89\n",
      "Iteration 3600: with minibatch training loss = 0.139 and accuracy of 0.95\n",
      "Iteration 3700: with minibatch training loss = 0.135 and accuracy of 0.98\n",
      "Iteration 3800: with minibatch training loss = 0.339 and accuracy of 0.89\n",
      "Epoch 5, Overall loss = 0.24 and accuracy of 0.919\n",
      "Iteration 3900: with minibatch training loss = 0.177 and accuracy of 0.94\n",
      "Iteration 4000: with minibatch training loss = 0.181 and accuracy of 0.94\n",
      "Iteration 4100: with minibatch training loss = 0.184 and accuracy of 0.92\n",
      "Iteration 4200: with minibatch training loss = 0.392 and accuracy of 0.83\n",
      "Iteration 4300: with minibatch training loss = 0.188 and accuracy of 0.92\n",
      "Iteration 4400: with minibatch training loss = 0.0844 and accuracy of 0.97\n",
      "Iteration 4500: with minibatch training loss = 0.0905 and accuracy of 0.97\n",
      "Epoch 6, Overall loss = 0.187 and accuracy of 0.935\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.89 and accuracy of 0.648\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 3.47 and accuracy of 0.16\n",
      "Iteration 100: with minibatch training loss = 1.62 and accuracy of 0.48\n",
      "Iteration 200: with minibatch training loss = 1.34 and accuracy of 0.53\n",
      "Iteration 300: with minibatch training loss = 1.34 and accuracy of 0.5\n",
      "Iteration 400: with minibatch training loss = 1.19 and accuracy of 0.58\n",
      "Iteration 500: with minibatch training loss = 1.06 and accuracy of 0.66\n",
      "Iteration 600: with minibatch training loss = 0.933 and accuracy of 0.66\n",
      "Iteration 700: with minibatch training loss = 0.828 and accuracy of 0.73\n",
      "Epoch 1, Overall loss = 1.38 and accuracy of 0.52\n",
      "Iteration 800: with minibatch training loss = 0.843 and accuracy of 0.72\n",
      "Iteration 900: with minibatch training loss = 0.798 and accuracy of 0.66\n",
      "Iteration 1000: with minibatch training loss = 1.01 and accuracy of 0.66\n",
      "Iteration 1100: with minibatch training loss = 0.833 and accuracy of 0.7\n",
      "Iteration 1200: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 1300: with minibatch training loss = 0.615 and accuracy of 0.77\n",
      "Iteration 1400: with minibatch training loss = 0.676 and accuracy of 0.77\n",
      "Iteration 1500: with minibatch training loss = 0.864 and accuracy of 0.64\n",
      "Epoch 2, Overall loss = 0.812 and accuracy of 0.723\n",
      "Iteration 1600: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 1700: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 1800: with minibatch training loss = 0.454 and accuracy of 0.91\n",
      "Iteration 1900: with minibatch training loss = 0.558 and accuracy of 0.81\n",
      "Iteration 2000: with minibatch training loss = 0.32 and accuracy of 0.92\n",
      "Iteration 2100: with minibatch training loss = 0.253 and accuracy of 0.94\n",
      "Iteration 2200: with minibatch training loss = 0.408 and accuracy of 0.84\n",
      "Epoch 3, Overall loss = 0.524 and accuracy of 0.83\n",
      "Iteration 2300: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 2400: with minibatch training loss = 0.363 and accuracy of 0.91\n",
      "Iteration 2500: with minibatch training loss = 0.415 and accuracy of 0.89\n",
      "Iteration 2600: with minibatch training loss = 0.188 and accuracy of 0.94\n",
      "Iteration 2700: with minibatch training loss = 0.355 and accuracy of 0.91\n",
      "Iteration 2800: with minibatch training loss = 0.286 and accuracy of 0.92\n",
      "Iteration 2900: with minibatch training loss = 0.284 and accuracy of 0.91\n",
      "Iteration 3000: with minibatch training loss = 0.25 and accuracy of 0.89\n",
      "Epoch 4, Overall loss = 0.303 and accuracy of 0.907\n",
      "Iteration 3100: with minibatch training loss = 0.256 and accuracy of 0.94\n",
      "Iteration 3200: with minibatch training loss = 0.182 and accuracy of 0.95\n",
      "Iteration 3300: with minibatch training loss = 0.247 and accuracy of 0.91\n",
      "Iteration 3400: with minibatch training loss = 0.0748 and accuracy of 0.98\n",
      "Iteration 3500: with minibatch training loss = 0.103 and accuracy of 0.98\n",
      "Iteration 3600: with minibatch training loss = 0.075 and accuracy of 0.98\n",
      "Iteration 3700: with minibatch training loss = 0.154 and accuracy of 0.94\n",
      "Iteration 3800: with minibatch training loss = 0.303 and accuracy of 0.89\n",
      "Epoch 5, Overall loss = 0.176 and accuracy of 0.947\n",
      "Iteration 3900: with minibatch training loss = 0.126 and accuracy of 0.95\n",
      "Iteration 4000: with minibatch training loss = 0.149 and accuracy of 0.97\n",
      "Iteration 4100: with minibatch training loss = 0.281 and accuracy of 0.94\n",
      "Iteration 4200: with minibatch training loss = 0.14 and accuracy of 0.97\n",
      "Iteration 4300: with minibatch training loss = 0.0709 and accuracy of 0.98\n",
      "Iteration 4400: with minibatch training loss = 0.126 and accuracy of 0.95\n",
      "Iteration 4500: with minibatch training loss = 0.108 and accuracy of 0.97\n",
      "Epoch 6, Overall loss = 0.131 and accuracy of 0.958\n",
      "Validation\n",
      "Epoch 1, Overall loss = 1.67 and accuracy of 0.66\n",
      "0.791 1e-06 0.001\n"
     ]
    }
   ],
   "source": [
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "regs = [1e-5, 1e-6, 1e-7]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "# learning_rates = [5e-3, 1e-3, 5e-4]\n",
    "best_val = -1\n",
    "best_reg = None\n",
    "best_lr = None\n",
    "for lr in learning_rates:\n",
    "    for reg in regs:\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "        X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int64, [None])\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        y_out = my_model(X,y,is_training)\n",
    "        \n",
    "        mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_out, labels=tf.one_hot(y,10)))\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "        mean_loss += reg * l2_loss\n",
    "        \n",
    "        train_step = optimizer.minimize(mean_loss)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,mean_loss,X_train,y_train,6,64,100,train_step)\n",
    "        print('Validation')\n",
    "        _, accu = run_model(sess,y_out,mean_loss,X_val,y_val,1,64)\n",
    "        if accu > best_val:\n",
    "            best_val, best_reg, best_lr = accu, reg, lr\n",
    "print(best_val, best_reg, best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.88 and accuracy of 0.094\n",
      "Iteration 100: with minibatch training loss = 1.98 and accuracy of 0.31\n",
      "Iteration 200: with minibatch training loss = 1.64 and accuracy of 0.45\n",
      "Iteration 300: with minibatch training loss = 1.46 and accuracy of 0.55\n",
      "Iteration 400: with minibatch training loss = 1.25 and accuracy of 0.59\n",
      "Iteration 500: with minibatch training loss = 1.01 and accuracy of 0.64\n",
      "Iteration 600: with minibatch training loss = 0.792 and accuracy of 0.78\n",
      "Iteration 700: with minibatch training loss = 0.999 and accuracy of 0.62\n",
      "Epoch 1, Overall loss = 1.24 and accuracy of 0.568\n",
      "Iteration 800: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 900: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 1000: with minibatch training loss = 0.856 and accuracy of 0.67\n",
      "Iteration 1100: with minibatch training loss = 0.859 and accuracy of 0.69\n",
      "Iteration 1200: with minibatch training loss = 0.655 and accuracy of 0.73\n",
      "Iteration 1300: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 1400: with minibatch training loss = 0.577 and accuracy of 0.83\n",
      "Iteration 1500: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Epoch 2, Overall loss = 0.729 and accuracy of 0.753\n",
      "Iteration 1600: with minibatch training loss = 0.559 and accuracy of 0.8\n",
      "Iteration 1700: with minibatch training loss = 0.743 and accuracy of 0.72\n",
      "Iteration 1800: with minibatch training loss = 0.569 and accuracy of 0.75\n",
      "Iteration 1900: with minibatch training loss = 0.508 and accuracy of 0.8\n",
      "Iteration 2000: with minibatch training loss = 0.425 and accuracy of 0.83\n",
      "Iteration 2100: with minibatch training loss = 0.621 and accuracy of 0.75\n",
      "Iteration 2200: with minibatch training loss = 0.557 and accuracy of 0.86\n",
      "Epoch 3, Overall loss = 0.531 and accuracy of 0.824\n",
      "Iteration 2300: with minibatch training loss = 0.327 and accuracy of 0.89\n",
      "Iteration 2400: with minibatch training loss = 0.363 and accuracy of 0.88\n",
      "Iteration 2500: with minibatch training loss = 0.341 and accuracy of 0.83\n",
      "Iteration 2600: with minibatch training loss = 0.315 and accuracy of 0.91\n",
      "Iteration 2700: with minibatch training loss = 0.431 and accuracy of 0.84\n",
      "Iteration 2800: with minibatch training loss = 0.366 and accuracy of 0.91\n",
      "Iteration 2900: with minibatch training loss = 0.325 and accuracy of 0.91\n",
      "Iteration 3000: with minibatch training loss = 0.278 and accuracy of 0.92\n",
      "Epoch 4, Overall loss = 0.38 and accuracy of 0.878\n",
      "Iteration 3100: with minibatch training loss = 0.313 and accuracy of 0.92\n",
      "Iteration 3200: with minibatch training loss = 0.287 and accuracy of 0.92\n",
      "Iteration 3300: with minibatch training loss = 0.246 and accuracy of 0.91\n",
      "Iteration 3400: with minibatch training loss = 0.355 and accuracy of 0.86\n",
      "Iteration 3500: with minibatch training loss = 0.205 and accuracy of 0.95\n",
      "Iteration 3600: with minibatch training loss = 0.3 and accuracy of 0.91\n",
      "Iteration 3700: with minibatch training loss = 0.131 and accuracy of 0.97\n",
      "Iteration 3800: with minibatch training loss = 0.216 and accuracy of 0.92\n",
      "Epoch 5, Overall loss = 0.275 and accuracy of 0.913\n",
      "Training\n",
      "Epoch 1, Overall loss = 0.337 and accuracy of 0.894\n",
      "Validation\n",
      "Epoch 1, Overall loss = 0.927 and accuracy of 0.786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.92725273895263671, 0.78600000000000003)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 70% accuracy on Validation\n",
    "\n",
    "tf.reset_default_graph()\n",
    "optimizer = tf.train.RMSPropOptimizer(best_lr)\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "y_out = my_model(X,y,is_training)\n",
    "\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_out, labels=tf.one_hot(y,10)))\n",
    "l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "mean_loss += best_reg * l2_loss\n",
    "\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,5,64,100,train_step)\n",
    "\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did here\n",
    "In this cell you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just choose a good-looking network and fine tune hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - Do this only once\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Epoch 1, Overall loss = 0.949 and accuracy of 0.769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.94913051776885982, 0.76900000000000002)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further with TensorFlow\n",
    "\n",
    "The next assignment will make heavy use of TensorFlow. You might also find it useful for your projects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
